"""
Vulnerability Analysis Module for the Grid Failure Modeling Framework.

This module analyzes and quantifies power grid component vulnerabilities
under various environmental conditions.
"""

import os
import json
import pickle
import logging
import datetime
from typing import Dict, List, Union, Optional, Any

from .component_profiler import ComponentProfiler
from .environmental_modeler import EnvironmentalThreatModeler
from .correlation_analyzer import CorrelationAnalyzer

# Configure logging
logger = logging.getLogger(__name__)

class VulnerabilityAnalysisModule:
    """
    Main class for the Vulnerability Analysis Module.
    
    This module integrates various components to analyze power grid vulnerabilities
    based on component characteristics and environmental conditions.
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """
        Initialize the Vulnerability Analysis Module.
        
        Args:
            config_path: Path to the configuration file. If None, uses default config.
        """
        # Load configuration
        self.config = self._load_config(config_path)
        
        # Setup logging
        self.logger = self._setup_logging()
        
        # Initialize data attributes
        self.grid_components = None
        self.weather_history = None
        self.outage_records = None
        self.combined_dataset = None
        
        # Paths for loading Module 1 data
        self.input_data_path = self.config.get('paths', {}).get(
            'input_data', 'data/processed/'
        )
        
        # Paths for saving Module 2 outputs
        self.output_data_path = self.config.get('paths', {}).get(
            'output_data', 'data/vulnerability_analysis/'
        )
        os.makedirs(self.output_data_path, exist_ok=True)
        
        # Initialize components
        self.component_profiler = ComponentProfiler(self.config)
        self.environmental_modeler = EnvironmentalThreatModeler(self.config)
        self.correlation_analyzer = CorrelationAnalyzer(self.config)
        
        logger.info("Vulnerability Analysis Module initialized")
    
    def _load_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """
        Load configuration from file.
        
        Args:
            config_path: Path to configuration file. If None, uses default config.
            
        Returns:
            Dict: Configuration dictionary
        """
        import yaml
        
        # Default configuration path
        if config_path is None:
            module_dir = os.path.dirname(os.path.abspath(__file__))
            config_path = os.path.join(module_dir, 'config', 'default_config.yaml')
        
        # Load configuration
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
                logger.info(f"Loaded configuration from {config_path}")
        except Exception as e:
            logger.warning(f"Error loading configuration from {config_path}: {e}")
            logger.info("Using default configuration")
            config = {
                'paths': {
                    'input_data': 'data/processed/',
                    'output_data': 'data/vulnerability_analysis/'
                },
                'component_profiling': {
                    'model_type': 'random_forest',
                    'test_size': 0.2,
                    'epochs': 100,
                    'batch_size': 64,
                    'learning_rate': 0.001,
                    'hidden_layers': [64, 32]
                },
                'environmental_threat': {
                    'threat_types': ['high_temperature', 'low_temperature', 'high_wind', 'precipitation'],
                    'threshold_percentiles': {
                        'high_temperature': 95,
                        'low_temperature': 5,
                        'high_wind': 95,
                        'precipitation': 95
                    },
                    'calculate_cumulative': True,
                    'time_lag_analysis': True,
                    'max_lag_days': 7
                },
                'correlation_analysis': {
                    'correlation_methods': ['pearson', 'spearman'],
                    'control_variables': ['season', 'time_of_day'],
                    'component_grouping': True,
                    'significance_level': 0.05,
                    'generate_visualizations': True
                },
                'logging': {
                    'level': 'INFO',
                    'file': 'logs/vulnerability_analysis/module.log'
                }
            }
        
        return config
    
    def _setup_logging(self) -> logging.Logger:
        """
        Set up logging for the module.
        
        Returns:
            logging.Logger: Configured logger
        """
        # Get logging configuration
        log_config = self.config.get('logging', {})
        log_level = getattr(logging, log_config.get('level', 'INFO'))
        log_file = log_config.get('file', 'logs/vulnerability_analysis/module.log')
        
        # Create log directory if it doesn't exist
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        
        # Configure logger
        logger = logging.getLogger(__name__)
        logger.setLevel(log_level)
        
        # Add file handler if not already present
        if not any(isinstance(h, logging.FileHandler) for h in logger.handlers):
            file_handler = logging.FileHandler(log_file)
            file_handler.setLevel(log_level)
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        
        return logger
    
    def load_input_data(self, use_synthetic: bool = False) -> Dict[str, Any]:
        """
        Load processed data from Module 1.
        
        Args:
            use_synthetic: Whether to load synthetic data.
                
        Returns:
            Dict: Dictionary containing loaded data
        """
        self.logger.info(f"Loading input data from {self.input_data_path}")
        
        if use_synthetic:
            input_path = os.path.join(self.input_data_path, 'synthetic')
        else:
            input_path = self.input_data_path
        
        # Load grid components data
        grid_components_path = os.path.join(input_path, 'grid_components.pkl')
        self.logger.info(f"Loading grid components from {grid_components_path}")
        try:
            with open(grid_components_path, 'rb') as f:
                self.grid_components = pickle.load(f)
            self.logger.info(f"Loaded {len(self.grid_components)} grid components")
        except Exception as e:
            self.logger.error(f"Failed to load grid components: {e}")
            raise
        
        # Load weather history data
        weather_history_path = os.path.join(input_path, 'weather_history.pkl')
        self.logger.info(f"Loading weather history from {weather_history_path}")
        try:
            with open(weather_history_path, 'rb') as f:
                self.weather_history = pickle.load(f)
            self.logger.info(f"Loaded {len(self.weather_history)} weather records")
        except Exception as e:
            self.logger.error(f"Failed to load weather history: {e}")
            raise
        
        # Load outage records data
        outage_records_path = os.path.join(input_path, 'outage_records.pkl')
        self.logger.info(f"Loading outage records from {outage_records_path}")
        try:
            with open(outage_records_path, 'rb') as f:
                self.outage_records = pickle.load(f)
            self.logger.info(f"Loaded {len(self.outage_records)} outage records")
        except Exception as e:
            self.logger.error(f"Failed to load outage records: {e}")
            raise
        
        # Load combined dataset
        combined_dataset_path = os.path.join(input_path, 'combined_dataset.pkl')
        self.logger.info(f"Loading combined dataset from {combined_dataset_path}")
        try:
            with open(combined_dataset_path, 'rb') as f:
                self.combined_dataset = pickle.load(f)
            self.logger.info(f"Loaded combined dataset with {len(self.combined_dataset)} records")
        except Exception as e:
            self.logger.error(f"Failed to load combined dataset: {e}")
            raise
        
        # Validate loaded data
        self._validate_input_data()
        
        return {
            'grid_components': self.grid_components,
            'weather_history': self.weather_history,
            'outage_records': self.outage_records,
            'combined_dataset': self.combined_dataset
        }
    
    def _validate_input_data(self):
        """Validate input data from Module 1"""
        # Check required columns in grid_components
        required_grid_cols = [
            'component_id', 'component_type'
        ]
        self._check_required_columns(self.grid_components, required_grid_cols)
        
        # Check required columns in weather_history
        required_weather_cols = [
            'timestamp', 'temperature'
        ]
        self._check_required_columns(self.weather_history, required_weather_cols)
        
        # Check required columns in outage_records
        required_outage_cols = [
            'component_id', 'start_time'
        ]
        self._check_required_columns(self.outage_records, required_outage_cols)
        
        self.logger.info("Input data validation completed successfully")
    
    def _check_required_columns(self, df, required_columns):
        """Check if DataFrame has required columns"""
        missing_cols = [col for col in required_columns if col not in df.columns]
        if missing_cols:
            error_msg = f"Missing required columns: {missing_cols}"
            self.logger.error(error_msg)
            raise ValueError(error_msg)
    
    def run_analysis(self, use_synthetic: bool = False) -> Dict[str, Any]:
        """
        Run the full vulnerability analysis pipeline.
        
        Args:
            use_synthetic: Whether to use synthetic data.
                
        Returns:
            Dict: Dictionary containing analysis results
        """
        # Load data from Module 1
        input_data = self.load_input_data(use_synthetic)
        
        # Component Vulnerability Profiling
        self.logger.info("Starting Component Vulnerability Profiling")
        vulnerability_scores = self.component_profiler.analyze(
            input_data['grid_components'],
            input_data['outage_records'],
            save_path=os.path.join(self.output_data_path, 'component_profiling')
        )
        self.logger.info(f"Completed vulnerability profiling for {len(vulnerability_scores)} components")
        
        # Environmental Threat Modeling
        self.logger.info("Starting Environmental Threat Modeling")
        threat_profiles = self.environmental_modeler.analyze(
            input_data['weather_history'],
            input_data['outage_records'],
            save_path=os.path.join(self.output_data_path, 'environmental_modeling')
        )
        self.logger.info("Completed environmental threat modeling")
        
        # Correlation Analysis
        self.logger.info("Starting Correlation Analysis")
        correlation_matrices = self.correlation_analyzer.analyze(
            vulnerability_scores,
            threat_profiles,
            input_data['weather_history'],
            input_data['outage_records'],
            save_path=os.path.join(self.output_data_path, 'correlation_analysis')
        )
        self.logger.info("Completed correlation analysis")
        
        # Compile all results
        analysis_results = {
            'vulnerability_scores': vulnerability_scores,
            'threat_profiles': threat_profiles,
            'correlation_matrices': correlation_matrices
        }
        
        # Save compiled results
        self._save_compiled_results(analysis_results)
        
        return analysis_results
    
    def _save_compiled_results(self, results: Dict[str, Any]):
        """
        Save compiled analysis results.
        
        Args:
            results: Dictionary containing analysis results
        """
        compiled_path = os.path.join(self.output_data_path, 'compiled_results.pkl')
        with open(compiled_path, 'wb') as f:
            pickle.dump(results, f)
        
        self.logger.info(f"Saved compiled vulnerability analysis results to {compiled_path}")
        
        # Create summary file for quick reference
        summary_path = os.path.join(self.output_data_path, 'analysis_summary.json')
        summary = {
            'component_count': len(results['vulnerability_scores']),
            'high_vulnerability_count': int(sum(
                results['vulnerability_scores']['vulnerability_score'] > 0.7
            )),
            'threat_types_analyzed': list(results['threat_profiles'].keys()),
            'correlation_types': list(results['correlation_matrices'].keys()),
            'analysis_timestamp': datetime.datetime.now().isoformat()
        }
        
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        self.logger.info(f"Saved analysis summary to {summary_path}")


def load_vulnerability_analysis_outputs(base_path='data/vulnerability_analysis/'):
    """
    Load outputs from Vulnerability Analysis Module.
    
    Args:
        base_path: Base path for vulnerability analysis outputs.
            
    Returns:
        Dict: Dictionary containing loaded outputs
    """
    # Load compiled results (all outputs in one file)
    compiled_path = os.path.join(base_path, 'compiled_results.pkl')
    try:
        with open(compiled_path, 'rb') as f:
            compiled_results = pickle.load(f)
            return compiled_results
    except FileNotFoundError:
        logger.warning(f"Compiled results not found at {compiled_path}, loading individual files")
    
    # Load individual output files
    results = {}
    
    # Load vulnerability scores
    scores_path = os.path.join(base_path, 'component_profiling/component_vulnerability_scores.pkl')
    try:
        with open(scores_path, 'rb') as f:
            results['vulnerability_scores'] = pickle.load(f)
    except FileNotFoundError:
        logger.warning(f"Vulnerability scores not found at {scores_path}")
    
    # Load environmental threat profiles
    profiles_path = os.path.join(base_path, 'environmental_modeling/environmental_threat_profiles.pkl')
    try:
        with open(profiles_path, 'rb') as f:
            results['threat_profiles'] = pickle.load(f)
    except FileNotFoundError:
        logger.warning(f"Environmental threat profiles not found at {profiles_path}")
    
    # Load correlation matrices
    matrices_path = os.path.join(base_path, 'correlation_analysis/correlation_matrices.pkl')
    try:
        with open(matrices_path, 'rb') as f:
            results['correlation_matrices'] = pickle.load(f)
    except FileNotFoundError:
        logger.warning(f"Correlation matrices not found at {matrices_path}")
    
    return results

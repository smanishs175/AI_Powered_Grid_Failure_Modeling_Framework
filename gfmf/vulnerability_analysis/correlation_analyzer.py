"""
Correlation analysis for the Grid Failure Modeling Framework.

This module analyzes correlations between environmental conditions,
component vulnerabilities, and outage events.
"""

import os
import numpy as np
import pandas as pd
import pickle
import logging
from typing import Dict, List, Union, Optional, Any, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta

# Import utility functions
from .utils.statistical_tools import (
    calculate_correlation,
    calculate_partial_correlation,
    grouped_statistical_test
)
from .utils.visualization import (
    plot_correlation_matrix
)
from .utils.validators import (
    validate_weather_history,
    validate_outage_records,
    validate_input_data_compatibility
)

# Configure logging
logger = logging.getLogger(__name__)

class CorrelationAnalyzer:
    """
    Correlation analysis module.
    
    This class analyzes correlations between environmental conditions,
    component vulnerabilities, and outage events.
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize correlation analyzer.
        
        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Get correlation analysis config
        self.correlation_config = config.get('correlation_analysis', {})
        
        # Analysis configuration
        self.correlation_methods = self.correlation_config.get(
            'correlation_methods', ['pearson', 'spearman']
        )
        
        self.control_variables = self.correlation_config.get(
            'control_variables', ['season', 'time_of_day']
        )
        
        self.component_grouping = self.correlation_config.get(
            'component_grouping', True
        )
        
        self.significance_level = self.correlation_config.get(
            'significance_level', 0.05
        )
        
        self.generate_visualizations = self.correlation_config.get(
            'generate_visualizations', True
        )
        
        # Initialize attributes
        self.correlation_matrices = {}
        self.partial_correlation_matrices = {}
        self.significance_matrices = {}
        self.grouped_correlations = {}
        
        self.logger.info("CorrelationAnalyzer initialized")
    
    def analyze(
        self,
        vulnerability_scores: pd.DataFrame,
        environmental_threats: Dict[str, pd.DataFrame],
        weather_history: pd.DataFrame,
        outage_records: pd.DataFrame,
        save_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Analyze correlations between environmental conditions and vulnerabilities.
        
        Args:
            vulnerability_scores: DataFrame with component vulnerability scores
            environmental_threats: Dictionary with environmental threat profiles
            weather_history: DataFrame with weather history
            outage_records: DataFrame with outage records
            save_path: Path to save results
            
        Returns:
            Dict: Dictionary with correlation analysis results
        """
        self.logger.info("Starting correlation analysis")
        
        # Create combined dataset for correlation analysis
        combined_data = self._create_combined_dataset(
            vulnerability_scores,
            environmental_threats,
            weather_history,
            outage_records
        )
        
        # Calculate correlation matrices
        self.correlation_matrices = self._calculate_correlation_matrices(
            combined_data['daily']
        )
        
        # Calculate partial correlations if control variables are specified
        if self.control_variables:
            self.partial_correlation_matrices = self._calculate_partial_correlations(
                combined_data['daily']
            )
        
        # Calculate grouped correlations if configured
        if self.component_grouping and 'component_type' in vulnerability_scores.columns:
            self.grouped_correlations = self._calculate_grouped_correlations(
                combined_data['daily'],
                vulnerability_scores
            )
        
        # Generate visualizations if configured
        if self.generate_visualizations and save_path:
            self._generate_visualizations(save_path)
        
        # Save results if path provided
        if save_path:
            os.makedirs(save_path, exist_ok=True)
            self._save_results(save_path)
        
        self.logger.info("Completed correlation analysis")
        
        # Return combined results
        results = {
            'correlation_matrices': self.correlation_matrices,
            'partial_correlation_matrices': self.partial_correlation_matrices,
            'significance_matrices': self.significance_matrices,
            'grouped_correlations': self.grouped_correlations,
            'combined_data': combined_data
        }
        
        return results
    
    def _create_combined_dataset(
        self,
        vulnerability_scores: pd.DataFrame,
        environmental_threats: Dict[str, pd.DataFrame],
        weather_history: pd.DataFrame,
        outage_records: pd.DataFrame
    ) -> Dict[str, pd.DataFrame]:
        """
        Create combined dataset for correlation analysis.
        
        Args:
            vulnerability_scores: DataFrame with component vulnerability scores
            environmental_threats: Dictionary with environmental threat profiles
            weather_history: DataFrame with weather history
            outage_records: DataFrame with outage records
            
        Returns:
            Dict: Dictionary with combined datasets at different time scales
        """
        self.logger.info("Creating combined dataset for correlation analysis")
        
        # Ensure timestamps are datetime
        weather_history['timestamp'] = pd.to_datetime(weather_history['timestamp'])
        outage_records['start_time'] = pd.to_datetime(outage_records['start_time'])
        
        # Create daily weather aggregates
        daily_weather = weather_history.copy()
        daily_weather['date'] = daily_weather['timestamp'].dt.date
        
        # Calculate daily weather statistics
        weather_vars = [
            'temperature', 'precipitation', 'wind_speed', 'humidity'
        ]
        
        daily_weather_stats = {}
        for var in weather_vars:
            if var in daily_weather.columns:
                # Calculate daily aggregates
                daily_weather_stats[f'{var}_max'] = daily_weather.groupby('date')[var].max()
                daily_weather_stats[f'{var}_min'] = daily_weather.groupby('date')[var].min()
                daily_weather_stats[f'{var}_mean'] = daily_weather.groupby('date')[var].mean()
        
        # Create daily outage counts
        daily_outages = outage_records.copy()
        daily_outages['date'] = daily_outages['start_time'].dt.date
        outage_count = daily_outages.groupby('date').size().rename('outage_count')
        
        # Get component types if available
        if 'component_type' in outage_records.columns:
            type_outage_counts = pd.DataFrame()
            for comp_type in outage_records['component_type'].unique():
                type_df = daily_outages[daily_outages['component_type'] == comp_type]
                type_count = type_df.groupby('date').size().rename(f'outage_{comp_type}')
                if type_outage_counts.empty:
                    type_outage_counts = pd.DataFrame(type_count)
                else:
                    type_outage_counts = type_outage_counts.join(
                        pd.DataFrame(type_count), how='outer'
                    )
            
            # Fill missing values with 0
            type_outage_counts = type_outage_counts.fillna(0)
        else:
            type_outage_counts = pd.DataFrame()
        
        # Get vulnerability metrics
        vulnerability_metrics = {}
        if not vulnerability_scores.empty:
            # Overall vulnerability statistics
            vulnerability_metrics['mean_vulnerability'] = \
                vulnerability_scores['vulnerability_score'].mean()
            vulnerability_metrics['high_vulnerability_count'] = \
                (vulnerability_scores['vulnerability_score'] > 0.7).sum()
            
            # Vulnerability by component type if available
            if 'component_type' in vulnerability_scores.columns:
                for comp_type in vulnerability_scores['component_type'].unique():
                    type_df = vulnerability_scores[vulnerability_scores['component_type'] == comp_type]
                    vulnerability_metrics[f'vulnerability_{comp_type}'] = \
                        type_df['vulnerability_score'].mean()
        
        # Get environmental threat metrics
        env_metrics = {}
        for threat_type, threat_df in environmental_threats.items():
            # Convert timestamp to datetime if not already
            if 'timestamp' in threat_df.columns:
                threat_df['timestamp'] = pd.to_datetime(threat_df['timestamp'])
                threat_df['date'] = threat_df['timestamp'].dt.date
                
                # Calculate daily threat statistics
                env_metrics[f'{threat_type}_max'] = threat_df.groupby('date')['threat_level'].max()
                env_metrics[f'{threat_type}_mean'] = threat_df.groupby('date')['threat_level'].mean()
                
                # Calculate duration of threats
                exceeds_df = threat_df[threat_df['exceeds_threshold'] > 0]
                if not exceeds_df.empty:
                    exceeds_df = exceeds_df.sort_values('timestamp')
                    
                    # Convert to datetime index and resample to calculate hours per day
                    exceeds_df = exceeds_df.set_index('timestamp')
                    daily_hours = exceeds_df['exceeds_threshold'].resample('D').sum() / 24  # Assuming hourly data
                    env_metrics[f'{threat_type}_duration'] = daily_hours
        
        # Create combined daily dataset
        # Get all dates from all datasets
        all_dates = set()
        all_dates.update([idx for metrics in daily_weather_stats.values() for idx in metrics.index])
        all_dates.update(outage_count.index)
        all_dates.update([idx for metrics in env_metrics.values() for idx in metrics.index])
        
        # Convert all dates to pandas Timestamps for consistent comparison
        standardized_dates = [pd.Timestamp(date) if not isinstance(date, pd.Timestamp) else date for date in all_dates]
        
        # Create DataFrame with all dates
        daily_df = pd.DataFrame(index=sorted(standardized_dates))
        daily_df.index.name = 'date'
        
        # Add outage counts
        daily_df['outage_count'] = outage_count
        
        # Add component-specific outage counts if available
        if not type_outage_counts.empty:
            for col in type_outage_counts.columns:
                daily_df[col] = type_outage_counts[col]
        
        # Add weather statistics
        for var_name, var_series in daily_weather_stats.items():
            daily_df[var_name] = var_series
        
        # Add environmental metrics
        for metric_name, metric_series in env_metrics.items():
            daily_df[metric_name] = metric_series
        
        # Add derived features
        
        # 1. Add day of week, month, season
        daily_df['day_of_week'] = pd.to_datetime(daily_df.index).dayofweek
        daily_df['month'] = pd.to_datetime(daily_df.index).month
        
        # Create season based on month
        def get_season(month):
            if month in [12, 1, 2]:
                return 'winter'
            elif month in [3, 4, 5]:
                return 'spring'
            elif month in [6, 7, 8]:
                return 'summer'
            else:
                return 'fall'
        
        daily_df['season'] = daily_df['month'].apply(get_season)
        
        # 2. Add lag features for weather variables
        weather_vars_to_lag = [
            col for col in daily_df.columns if any(
                var in col for var in [
                    'temperature', 'precipitation', 'wind_speed', 
                    'threat_level', 'exceeds_threshold'
                ]
            )
        ]
        
        for var in weather_vars_to_lag:
            if var in daily_df.columns:
                # Add 1-day and 3-day lags
                daily_df[f'{var}_lag1'] = daily_df[var].shift(1)
                daily_df[f'{var}_lag3'] = daily_df[var].shift(3)
                
                # Add rolling averages (7-day window)
                daily_df[f'{var}_roll7'] = daily_df[var].rolling(window=7, min_periods=1).mean()
        
        # Fill missing values
        daily_df = daily_df.fillna(0)
        
        # Create correlation-ready dataset
        corr_df = daily_df.copy()
        
        # Add categorical features for correlation analysis
        if 'season' in corr_df.columns:
            # Convert season to dummy variables
            season_dummies = pd.get_dummies(corr_df['season'], prefix='season')
            corr_df = pd.concat([corr_df, season_dummies], axis=1)
        
        if 'day_of_week' in corr_df.columns:
            # Create weekend flag
            corr_df['is_weekend'] = (corr_df['day_of_week'] >= 5).astype(int)
        
        # Create monthly dataset (resampled) - only for numeric columns
        monthly_df = daily_df.copy()
        monthly_df.index = pd.to_datetime(monthly_df.index)
        
        # Identify non-numeric columns that we need to handle differently
        numeric_cols = monthly_df.select_dtypes(include=['number']).columns.tolist()
        category_cols = monthly_df.select_dtypes(exclude=['number']).columns.tolist()
        
        # Only apply resampling to numeric columns
        if numeric_cols:
            numeric_monthly = monthly_df[numeric_cols].resample('M').mean()
            
            # Create a new DataFrame with the resampled data
            result_monthly = numeric_monthly.copy()
            
            # For non-numeric columns, we'll use the most frequent value for each month
            if category_cols:
                self.logger.info(f"Handling {len(category_cols)} non-numeric columns during resampling: {category_cols}")
                for col in category_cols:
                    try:
                        # Save the column data before resampling
                        col_data = monthly_df[[col]].copy()
                        
                        # Group by month and get the most common value
                        col_data['year_month'] = col_data.index.to_period('M')
                        mode_values = col_data.groupby('year_month')[col].apply(
                            lambda x: x.mode()[0] if not x.mode().empty else None
                        )
                        
                        # Add the mode values back with timestamps that match the numeric data
                        for idx in numeric_monthly.index:
                            period = pd.Period(idx, freq='M')
                            if period in mode_values.index:
                                result_monthly.loc[idx, col] = mode_values[period]
                    except Exception as e:
                        self.logger.warning(f"Error handling non-numeric column {col}: {e}")
            
            monthly_df = result_monthly
        else:
            # If there are no numeric columns, just use the first day of each month
            self.logger.warning("No numeric columns found for monthly resampling")
            monthly_df = monthly_df.resample('M').first()
        
        return {
            'daily': corr_df,
            'monthly': monthly_df,
            'vulnerability_metrics': vulnerability_metrics
        }
    
    def _calculate_correlation_matrices(
        self,
        combined_data: pd.DataFrame
    ) -> Dict[str, pd.DataFrame]:
        """
        Calculate correlation matrices between variables.
        
        Args:
            combined_data: DataFrame with combined data
            
        Returns:
            Dict: Dictionary with correlation matrices for each method
        """
        self.logger.info("Calculating correlation matrices")
        
        correlation_matrices = {}
        significance_matrices = {}
        
        # Calculate correlations for each method
        for method in self.correlation_methods:
            # Filter out non-numeric columns that would cause errors
            numeric_cols = combined_data.select_dtypes(include=['number']).columns
            numeric_data = combined_data[numeric_cols]
            
            # Log the excluded columns for debugging
            excluded_cols = set(combined_data.columns) - set(numeric_cols)
            if excluded_cols:
                self.logger.info(f"Excluding non-numeric columns from correlation calculation: {excluded_cols}")
            
            # Calculate correlation matrix with only numeric columns
            corr_matrix = numeric_data.corr(method=method)
            correlation_matrices[method] = corr_matrix
            
            # Calculate significance matrix
            p_values = pd.DataFrame(
                index=corr_matrix.index,
                columns=corr_matrix.columns,
                data=np.ones((len(corr_matrix), len(corr_matrix)))
            )
            
            # Calculate p-values for each pair
            for i, var1 in enumerate(corr_matrix.columns):
                for j, var2 in enumerate(corr_matrix.columns):
                    if i < j:  # Only calculate once per pair
                        # Skip if either variable has no variance
                        if numeric_data[var1].std() == 0 or numeric_data[var2].std() == 0:
                            continue
                            
                        _, p_val = calculate_correlation(
                            numeric_data[var1],
                            numeric_data[var2],
                            method=method
                        )
                        
                        p_values.loc[var1, var2] = p_val
                        p_values.loc[var2, var1] = p_val  # Matrix is symmetric
            
            significance_matrices[method] = p_values
        
        # Store significance matrices
        self.significance_matrices = significance_matrices
        
        return correlation_matrices
    
    def _calculate_partial_correlations(
        self,
        combined_data: pd.DataFrame
    ) -> Dict[str, pd.DataFrame]:
        """
        Calculate partial correlation matrices with control variables.
        
        Args:
            combined_data: DataFrame with combined data
            
        Returns:
            Dict: Dictionary with partial correlation matrices
        """
        self.logger.info("Calculating partial correlations")
        
        partial_matrices = {}
        
        # Check if control variables exist in data
        controls = [
            var for var in self.control_variables
            if var in combined_data.columns
        ]
        
        if not controls:
            self.logger.warning("No control variables found in data")
            return partial_matrices
        
        # Get variables of interest (exclude control variables)
        variables = [
            var for var in combined_data.columns
            if var not in controls and 'season_' not in var
        ]
        
        # Calculate partial correlations for outage count and weather/threat variables
        outage_vars = [var for var in variables if 'outage' in var]
        weather_vars = [
            var for var in variables if any(
                term in var for term in [
                    'temperature', 'precipitation', 'wind', 'threat'
                ]
            )
        ]
        
        for outage_var in outage_vars:
            # Create result matrices
            corr_matrix = pd.DataFrame(
                index=weather_vars,
                columns=['partial_corr', 'p_value']
            )
            
            for weather_var in weather_vars:
                # Calculate partial correlation
                corr, p_val = calculate_partial_correlation(
                    combined_data,
                    outage_var,
                    weather_var,
                    controls
                )
                
                corr_matrix.loc[weather_var, 'partial_corr'] = corr
                corr_matrix.loc[weather_var, 'p_value'] = p_val
            
            # Sort by absolute correlation
            corr_matrix = corr_matrix.reindex(
                corr_matrix['partial_corr'].abs().sort_values(ascending=False).index
            )
            
            partial_matrices[outage_var] = corr_matrix
        
        return partial_matrices
    
    def _calculate_grouped_correlations(
        self,
        combined_data: pd.DataFrame,
        vulnerability_scores: pd.DataFrame
    ) -> Dict[str, Any]:
        """
        Calculate correlations grouped by component types.
        
        Args:
            combined_data: DataFrame with combined data
            vulnerability_scores: DataFrame with component vulnerability scores
            
        Returns:
            Dict: Dictionary with grouped correlation results
        """
        self.logger.info("Calculating grouped correlations")
        
        grouped_results = {}
        
        # Check if component type information is available
        if 'component_type' not in vulnerability_scores.columns:
            self.logger.warning("Component type information not available")
            return grouped_results
        
        # Get unique component types
        component_types = vulnerability_scores['component_type'].unique()
        
        # Find outage variables for different component types
        outage_vars = [
            var for var in combined_data.columns
            if any(f'outage_{comp_type}' == var for comp_type in component_types)
        ]
        
        # Find environmental threat variables
        threat_vars = [
            var for var in combined_data.columns
            if '_threat_' in var or any(
                threat_type in var for threat_type in [
                    'high_temperature', 'low_temperature',
                    'high_wind', 'precipitation'
                ]
            )
        ]
        
        # Calculate correlations between component-specific outages and threats
        type_correlations = {}
        for outage_var in outage_vars:
            # Extract component type from variable name
            comp_type = outage_var.replace('outage_', '')
            
            # Calculate correlations with each threat variable
            corr_results = []
            for threat_var in threat_vars:
                # Calculate correlation
                corr, p_val = calculate_correlation(
                    combined_data[outage_var],
                    combined_data[threat_var],
                    method='spearman'
                )
                
                corr_results.append({
                    'threat_variable': threat_var,
                    'correlation': corr,
                    'p_value': p_val,
                    'significant': p_val < self.significance_level
                })
            
            # Sort by absolute correlation
            corr_results.sort(key=lambda x: abs(x['correlation']), reverse=True)
            
            type_correlations[comp_type] = corr_results
        
        # Perform statistical tests between vulnerability distributions
        if len(component_types) > 1:
            # Extract vulnerability scores by component type
            vuln_by_type = {}
            for comp_type in component_types:
                type_scores = vulnerability_scores[
                    vulnerability_scores['component_type'] == comp_type
                ]['vulnerability_score']
                
                vuln_by_type[comp_type] = type_scores
            
            # Create combined DataFrame for statistical test with explicit indices
            # Using reset_index(drop=True) to avoid mixing string and int indices
            vuln_scores = pd.concat(vuln_by_type.values()).reset_index(drop=True)
            comp_types = pd.concat([
                pd.Series([comp_type] * len(scores)) 
                for comp_type, scores in vuln_by_type.items()
            ]).reset_index(drop=True)
            
            combined_vuln = pd.DataFrame({
                'vulnerability_score': vuln_scores,
                'component_type': comp_types
            })
            
            # Perform statistical test
            test_results = grouped_statistical_test(
                combined_vuln,
                'component_type',
                'vulnerability_score',
                test='t-test'
            )
            
            grouped_results['vulnerability_comparison'] = test_results
        
        # Store type-specific correlations
        grouped_results['type_correlations'] = type_correlations
        
        return grouped_results
    
    def _generate_visualizations(self, save_path: str):
        """
        Generate visualizations for correlation analysis.
        
        Args:
            save_path: Path to save visualizations
        """
        # Create directory if it doesn't exist
        viz_dir = os.path.join(save_path, 'visualizations')
        os.makedirs(viz_dir, exist_ok=True)
        
        # 1. Correlation heatmaps
        for method, corr_matrix in self.correlation_matrices.items():
            self.logger.info(f"Generating correlation heatmap for {method} method")
            
            # Filter variables of interest
            outage_vars = [col for col in corr_matrix.columns if 'outage' in col]
            env_vars = [
                col for col in corr_matrix.columns
                if any(term in col for term in [
                    'temperature', 'precipitation', 'wind', 'threat'
                ]) and not any(term in col for term in ['lag', 'roll'])
            ]
            
            # Limit to top correlated variables
            top_env_vars = []
            for outage_var in outage_vars:
                # Get top correlated variables for this outage variable
                top_for_var = corr_matrix.loc[env_vars, outage_var].abs().nlargest(5).index.tolist()
                top_env_vars.extend(top_for_var)
            
            # Get unique variables
            top_env_vars = list(set(top_env_vars))
            
            # Create filtered correlation matrix
            filtered_vars = outage_vars + top_env_vars
            filtered_corr = corr_matrix.loc[filtered_vars, filtered_vars]
            
            # Get corresponding p-values if available
            p_values = None
            if method in self.significance_matrices:
                p_values = self.significance_matrices[method].loc[filtered_vars, filtered_vars]
            
            # Plot correlation matrix
            plot_correlation_matrix(
                filtered_corr,
                p_value_matrix=p_values,
                significance_level=self.significance_level,
                title=f'{method.capitalize()} Correlation Matrix',
                save_path=os.path.join(viz_dir, f'{method}_correlation_matrix.png')
            )
        
        # 2. Partial correlation bar charts
        for outage_var, partial_corr in self.partial_correlation_matrices.items():
            self.logger.info(f"Generating partial correlation chart for {outage_var}")
            
            # Filter to significant correlations
            sig_corr = partial_corr[partial_corr['p_value'] < self.significance_level]
            
            # Skip if no significant correlations
            if sig_corr.empty:
                continue
            
            # Create plot
            plt.figure(figsize=(12, 8))
            
            # Ensure values are numeric before applying colormap
            try:
                # Convert to numeric, coerce any non-convertible values to NaN
                partial_corrs = pd.to_numeric(sig_corr['partial_corr'], errors='coerce')
                # Replace any NaN with 0 to avoid colormap errors
                partial_corrs = partial_corrs.fillna(0)
                
                # Create normalized values for colormap (between 0 and 1)
                norm_values = (partial_corrs + 1) / 2
                colors = plt.cm.RdBu(norm_values.values)
                
                # Create bar chart
                plt.barh(
                    sig_corr.index,
                    partial_corrs,
                    color=colors
                )
            except Exception as e:
                # Fallback to a simple bar chart with a single color if there's any issue
                self.logger.warning(f"Error applying color gradient to partial correlation chart: {e}")
                plt.barh(
                    sig_corr.index,
                    sig_corr['partial_corr'],
                    color='skyblue'
                )
            
            # Add p-value annotations with proper error handling
            for i, (var, row) in enumerate(sig_corr.iterrows()):
                try:
                    # Convert values to numeric types safely
                    corr_val = float(row['partial_corr'])
                    p_val = float(row['p_value'])
                    
                    # Calculate annotation position
                    pos = corr_val + (0.01 if corr_val >= 0 else -0.01)
                    
                    # Add text annotation
                    plt.text(
                        pos,
                        i,
                        f"p={p_val:.3f}",
                        va='center'
                    )
                except (ValueError, TypeError) as e:
                    self.logger.warning(f"Error adding p-value annotation for {var}: {e}")
            
            # Customize plot
            plt.axvline(0, color='black', linestyle='-', alpha=0.3)
            plt.xlabel('Partial Correlation Coefficient')
            plt.title(f'Partial Correlations with {outage_var}\n(Controlling for {", ".join(self.control_variables)})')
            plt.grid(alpha=0.3)
            
            # Save plot
            plt.tight_layout()
            plt.savefig(
                os.path.join(viz_dir, f'{outage_var}_partial_correlations.png'),
                dpi=300,
                bbox_inches='tight'
            )
            plt.close()
    
    def _save_results(self, save_path: str):
        """
        Save analysis results.
        
        Args:
            save_path: Path to save results
        """
        # Create directory if it doesn't exist
        os.makedirs(save_path, exist_ok=True)
        
        # Save correlation matrices
        matrices_path = os.path.join(save_path, 'correlation_matrices.pkl')
        with open(matrices_path, 'wb') as f:
            pickle.dump(self.correlation_matrices, f)
        
        self.logger.info(f"Saved correlation matrices to {matrices_path}")
        
        # Save partial correlation matrices if available
        if self.partial_correlation_matrices:
            partial_path = os.path.join(save_path, 'partial_correlation_matrices.pkl')
            with open(partial_path, 'wb') as f:
                pickle.dump(self.partial_correlation_matrices, f)
            
            self.logger.info(f"Saved partial correlation matrices to {partial_path}")
        
        # Save grouped correlations if available
        if self.grouped_correlations:
            grouped_path = os.path.join(save_path, 'grouped_correlations.pkl')
            with open(grouped_path, 'wb') as f:
                pickle.dump(self.grouped_correlations, f)
            
            self.logger.info(f"Saved grouped correlations to {grouped_path}")
        
        # Create summary CSV files for easy viewing
        
        # 1. Top correlations with outages
        top_correlations = []
        for method, corr_matrix in self.correlation_matrices.items():
            outage_vars = [col for col in corr_matrix.columns if 'outage' in col]
            env_vars = [
                col for col in corr_matrix.columns
                if any(term in col for term in [
                    'temperature', 'precipitation', 'wind', 'threat'
                ])
            ]
            
            for outage_var in outage_vars:
                # Get correlations with this outage variable
                corrs = corr_matrix.loc[env_vars, outage_var].to_dict()
                
                # Get p-values if available
                p_values = {}
                if method in self.significance_matrices:
                    p_values = self.significance_matrices[method].loc[env_vars, outage_var].to_dict()
                
                # Add to results
                for env_var, corr in corrs.items():
                    p_val = p_values.get(env_var, float('nan'))
                    top_correlations.append({
                        'outage_variable': outage_var,
                        'environmental_variable': env_var,
                        'correlation_method': method,
                        'correlation': corr,
                        'p_value': p_val,
                        'significant': p_val < self.significance_level
                    })
        
        # Create DataFrame and sort
        top_corr_df = pd.DataFrame(top_correlations)
        if not top_corr_df.empty:
            top_corr_df = top_corr_df.sort_values(
                ['outage_variable', 'correlation_method', 'significant', 'correlation'],
                ascending=[True, True, False, False]
            )
            
            # Save to CSV
            top_corr_path = os.path.join(save_path, 'top_correlations.csv')
            top_corr_df.to_csv(top_corr_path, index=False)
            
            self.logger.info(f"Saved top correlations to {top_corr_path}")
    
    def load_correlation_matrices(self, load_path: str) -> Dict[str, pd.DataFrame]:
        """
        Load previously calculated correlation matrices.
        
        Args:
            load_path: Path to load matrices from
            
        Returns:
            Dict: Dictionary with correlation matrices
        """
        # Determine file path
        matrices_path = os.path.join(load_path, 'correlation_matrices.pkl')
        
        # Load correlation matrices
        try:
            with open(matrices_path, 'rb') as f:
                self.correlation_matrices = pickle.load(f)
            self.logger.info(f"Loaded correlation matrices from {matrices_path}")
        except Exception as e:
            self.logger.error(f"Error loading correlation matrices: {e}")
            raise
        
        return self.correlation_matrices

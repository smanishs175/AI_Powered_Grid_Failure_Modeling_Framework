"""
Statistical analysis tools for vulnerability analysis.

This module provides functions for statistical calculations and analysis
related to vulnerability assessment of power grid components.
"""

import numpy as np
import pandas as pd
from scipy import stats
from typing import Dict, List, Union, Optional, Any, Tuple
import logging

# Configure logging
logger = logging.getLogger(__name__)

def calculate_correlation(
    data1: Union[pd.Series, np.ndarray],
    data2: Union[pd.Series, np.ndarray],
    method: str = 'pearson'
) -> Tuple[float, float]:
    """
    Calculate correlation between two data series with p-value.
    
    Args:
        data1: First data series
        data2: Second data series
        method: Correlation method ('pearson', 'spearman', 'kendall')
        
    Returns:
        Tuple: (correlation coefficient, p-value)
    """
    # Convert to numpy arrays
    if isinstance(data1, pd.Series):
        data1 = data1.values
    if isinstance(data2, pd.Series):
        data2 = data2.values
    
    # Drop NaN values
    valid_indices = ~(np.isnan(data1) | np.isnan(data2))
    data1_clean = data1[valid_indices]
    data2_clean = data2[valid_indices]
    
    # Check if we have enough data
    if len(data1_clean) < 2:
        logger.warning("Not enough valid data points for correlation")
        return 0.0, 1.0
    
    # Calculate correlation
    if method == 'pearson':
        return stats.pearsonr(data1_clean, data2_clean)
    elif method == 'spearman':
        return stats.spearmanr(data1_clean, data2_clean)
    elif method == 'kendall':
        return stats.kendalltau(data1_clean, data2_clean)
    else:
        raise ValueError(f"Unsupported correlation method: {method}")

def calculate_partial_correlation(
    data: pd.DataFrame,
    var1: str,
    var2: str,
    control_vars: List[str]
) -> Tuple[float, float]:
    """
    Calculate partial correlation with control variables.
    
    Args:
        data: DataFrame containing variables
        var1: First variable name
        var2: Second variable name
        control_vars: List of control variable names
        
    Returns:
        Tuple: (partial correlation coefficient, p-value)
    """
    import statsmodels.api as sm
    
    # Check if we have any control variables
    if not control_vars:
        # If no control variables, calculate regular correlation
        return calculate_correlation(data[var1], data[var2])
    
    # Ensure we're only using numeric data
    try:
        # Check for non-numeric columns that need to be excluded
        all_vars = [var1, var2] + control_vars
        numeric_cols = data[all_vars].select_dtypes(include=['number']).columns.tolist()
        non_numeric = set(all_vars) - set(numeric_cols)
        
        if non_numeric:
            logger.warning(f"Excluding non-numeric variables from partial correlation: {non_numeric}")
            # Filter out non-numeric control variables
            control_vars = [var for var in control_vars if var in numeric_cols]
            
            # If either main variable is non-numeric, we can't calculate correlation
            if var1 not in numeric_cols or var2 not in numeric_cols:
                logger.warning(f"Cannot calculate partial correlation: Variable {var1 if var1 not in numeric_cols else var2} is non-numeric")
                return 0.0, 1.0
        
        # Drop rows with NaN values
        data_clean = data[[var1, var2] + control_vars].dropna()
        
        # Check if we have enough data
        if len(data_clean) < len(control_vars) + 2:
            logger.warning("Not enough valid data points for partial correlation")
            return 0.0, 1.0
            
        # Create design matrices for control variables
        if control_vars:  # Only add constant if we have control variables
            X = sm.add_constant(data_clean[control_vars])
        else:
            # If no control variables left after filtering, return regular correlation
            return calculate_correlation(data_clean[var1], data_clean[var2])
        
        # Fit linear models to predict each variable from control variables
        model1 = sm.OLS(data_clean[var1], X).fit()
        model2 = sm.OLS(data_clean[var2], X).fit()
    except Exception as e:
        logger.warning(f"Error in partial correlation calculation: {e}")
        return 0.0, 1.0
    
    # Calculate residuals
    resid1 = model1.resid
    resid2 = model2.resid
    
    # Calculate correlation on residuals
    return stats.pearsonr(resid1, resid2)

def calculate_time_lagged_correlation(
    time_series1: pd.Series,
    time_series2: pd.Series,
    max_lag: int = 7,
    method: str = 'pearson'
) -> Dict[int, Tuple[float, float]]:
    """
    Calculate time-lagged correlations between two time series.
    
    Args:
        time_series1: First time series
        time_series2: Second time series
        max_lag: Maximum time lag to analyze
        method: Correlation method ('pearson', 'spearman', 'kendall')
        
    Returns:
        Dict: Dictionary mapping lag to (correlation, p-value)
    """
    results = {}
    
    # Calculate correlation for each lag
    for lag in range(max_lag + 1):
        # Shift the second time series by lag
        lagged_series = time_series2.shift(lag)
        
        # Calculate correlation
        corr, p_value = calculate_correlation(time_series1, lagged_series, method)
        
        results[lag] = (corr, p_value)
    
    return results

def find_optimal_threshold(
    predictor: pd.Series,
    target: pd.Series,
    num_bins: int = 10,
    min_samples: int = 5
) -> Dict[str, Any]:
    """
    Find optimal threshold value that maximizes correlation with target.
    
    Args:
        predictor: Predictor variable
        target: Target variable
        num_bins: Number of bins for analysis
        min_samples: Minimum samples per bin
        
    Returns:
        Dict: Dictionary with optimal threshold and statistics
    """
    # Create bins for the predictor variable
    try:
        bins = pd.qcut(predictor, q=num_bins, duplicates='drop')
    except ValueError:
        # If qcut fails (e.g., with too many identical values)
        # Try to use cut instead
        bins = pd.cut(predictor, bins=num_bins)
    
    # Calculate average target value per bin
    bin_stats = target.groupby(bins).agg(['mean', 'std', 'count'])
    
    # Find the bin with maximum target value
    valid_bins = bin_stats[bin_stats['count'] >= min_samples]
    
    if valid_bins.empty:
        logger.warning("No valid bins with enough samples")
        return {
            'threshold': None,
            'avg_above': None,
            'avg_below': None,
            'ratio': None,
            'p_value': None
        }
    
    max_bin_idx = valid_bins['mean'].idxmax()
    
    # Calculate overall average
    overall_avg = target.mean()
    
    # Get the minimum value of the bin with maximum target value
    threshold = max_bin_idx.left
    
    # Calculate statistics for values above and below threshold
    above_threshold = target[predictor > threshold]
    below_threshold = target[predictor <= threshold]
    
    avg_above = above_threshold.mean() if not above_threshold.empty else None
    avg_below = below_threshold.mean() if not below_threshold.empty else None
    
    # Calculate ratio
    ratio = avg_above / avg_below if avg_below and avg_below > 0 else float('inf')
    
    # Calculate t-test for difference in means
    if not above_threshold.empty and not below_threshold.empty:
        t_stat, p_value = stats.ttest_ind(
            above_threshold,
            below_threshold,
            equal_var=False  # Welch's t-test
        )
    else:
        p_value = None
    
    return {
        'threshold': threshold,
        'avg_above': avg_above,
        'avg_below': avg_below,
        'ratio': ratio,
        'p_value': p_value
    }

def detect_outliers(
    data: pd.Series,
    method: str = 'iqr',
    threshold: float = 1.5
) -> pd.Series:
    """
    Detect outliers in a data series.
    
    Args:
        data: Data series
        method: Method for outlier detection ('iqr', 'zscore')
        threshold: Threshold for outlier detection
        
    Returns:
        pd.Series: Boolean series indicating outliers
    """
    if method == 'iqr':
        q1 = data.quantile(0.25)
        q3 = data.quantile(0.75)
        iqr = q3 - q1
        
        lower_bound = q1 - threshold * iqr
        upper_bound = q3 + threshold * iqr
        
        return (data < lower_bound) | (data > upper_bound)
    
    elif method == 'zscore':
        z_scores = np.abs((data - data.mean()) / data.std())
        return z_scores > threshold
    
    else:
        raise ValueError(f"Unsupported outlier detection method: {method}")

def bootstrap_confidence_interval(
    data: pd.Series,
    statistic: callable = np.mean,
    n_bootstrap: int = 1000,
    confidence: float = 0.95
) -> Tuple[float, float]:
    """
    Calculate bootstrap confidence interval for a statistic.
    
    Args:
        data: Data series
        statistic: Statistic function to bootstrap
        n_bootstrap: Number of bootstrap samples
        confidence: Confidence level
        
    Returns:
        Tuple: (lower bound, upper bound)
    """
    # Clean data
    data_clean = data.dropna()
    
    # Generate bootstrap samples
    bootstrap_statistics = []
    for _ in range(n_bootstrap):
        bootstrap_sample = np.random.choice(data_clean, size=len(data_clean), replace=True)
        bootstrap_statistics.append(statistic(bootstrap_sample))
    
    # Calculate confidence interval
    lower_percentile = 100 * (1 - confidence) / 2
    upper_percentile = 100 - lower_percentile
    
    lower_bound = np.percentile(bootstrap_statistics, lower_percentile)
    upper_bound = np.percentile(bootstrap_statistics, upper_percentile)
    
    return lower_bound, upper_bound

def grouped_statistical_test(
    df: pd.DataFrame,
    group_col: str,
    value_col: str,
    test: str = 't-test'
) -> pd.DataFrame:
    """
    Perform statistical tests between groups.
    
    Args:
        df: DataFrame with data
        group_col: Column name for grouping
        value_col: Column name for values to test
        test: Type of test ('t-test', 'mann-whitney')
        
    Returns:
        pd.DataFrame: Matrix of p-values between groups
    """
    groups = df[group_col].unique()
    n_groups = len(groups)
    
    # Initialize results matrix
    results = pd.DataFrame(
        index=groups,
        columns=groups,
        data=np.ones((n_groups, n_groups))  # Fill with ones (p-value of 1)
    )
    
    # Perform tests between all pairs of groups
    for i, group1 in enumerate(groups):
        values1 = df[df[group_col] == group1][value_col].dropna()
        
        for j, group2 in enumerate(groups):
            if i <= j:  # Skip redundant tests
                continue
                
            values2 = df[df[group_col] == group2][value_col].dropna()
            
            # Skip if not enough data
            if len(values1) < 2 or len(values2) < 2:
                continue
            
            # Perform test
            if test == 't-test':
                _, p_value = stats.ttest_ind(values1, values2, equal_var=False)
            elif test == 'mann-whitney':
                _, p_value = stats.mannwhitneyu(values1, values2)
            else:
                raise ValueError(f"Unsupported test: {test}")
            
            results.loc[group1, group2] = p_value
            results.loc[group2, group1] = p_value  # Symmetric matrix
    
    return results

def estimate_failure_probability(
    components_df: pd.DataFrame,
    failures_df: pd.DataFrame,
    time_window: int,
    component_id_col: str = 'component_id'
) -> pd.Series:
    """
    Estimate failure probability per component.
    
    Args:
        components_df: DataFrame with component information
        failures_df: DataFrame with failure information
        time_window: Time window in days for probability estimation
        component_id_col: Column name for component ID
        
    Returns:
        pd.Series: Estimated failure probability per component
    """
    # Count failures per component
    failure_counts = failures_df[component_id_col].value_counts()
    
    # Get unique component IDs
    component_ids = components_df[component_id_col].unique()
    
    # Initialize probability series with all components
    probability = pd.Series(0.0, index=component_ids)
    
    # Update with failure counts
    probability.update(failure_counts / time_window)
    
    return probability

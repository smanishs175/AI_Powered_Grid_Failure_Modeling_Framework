"""
Data validation utilities for vulnerability analysis.

This module provides functions for validating input data before analysis.
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Union, Optional, Any, Tuple

# Configure logging
logger = logging.getLogger(__name__)

def validate_grid_components(grid_components: pd.DataFrame) -> Tuple[bool, List[str]]:
    """
    Validate grid components data.
    
    Args:
        grid_components: DataFrame with grid component data
        
    Returns:
        Tuple: (is_valid, list of issues)
    """
    issues = []
    
    # Check if DataFrame is empty
    if grid_components.empty:
        issues.append("Grid components DataFrame is empty")
        return False, issues
    
    # Check required columns
    required_columns = ['component_id', 'component_type']
    missing_columns = [col for col in required_columns if col not in grid_components.columns]
    if missing_columns:
        issues.append(f"Missing required columns: {missing_columns}")
    
    # Check for duplicate component IDs
    if 'component_id' in grid_components.columns:
        duplicate_ids = grid_components['component_id'].duplicated()
        if duplicate_ids.any():
            issues.append(f"Found {duplicate_ids.sum()} duplicate component IDs")
    
    # Check for missing values in required fields
    for col in [c for c in required_columns if c in grid_components.columns]:
        missing_values = grid_components[col].isna().sum()
        if missing_values > 0:
            issues.append(f"Column '{col}' has {missing_values} missing values")
    
    # Check that component types are valid
    if 'component_type' in grid_components.columns:
        valid_types = ['generator', 'transformer', 'line', 'bus', 'substation', 'switch']
        invalid_types = set(grid_components['component_type'].unique()) - set(valid_types)
        if invalid_types:
            issues.append(f"Found invalid component types: {invalid_types}")
    
    # Check for location data if available
    location_columns = ['latitude', 'longitude']
    if all(col in grid_components.columns for col in location_columns):
        # Check latitude range
        if (grid_components['latitude'] < -90).any() or (grid_components['latitude'] > 90).any():
            issues.append("Latitude values outside valid range (-90 to 90)")
        
        # Check longitude range
        if (grid_components['longitude'] < -180).any() or (grid_components['longitude'] > 180).any():
            issues.append("Longitude values outside valid range (-180 to 180)")
    
    # Return validation result
    return len(issues) == 0, issues

def validate_weather_history(weather_history: pd.DataFrame) -> Tuple[bool, List[str]]:
    """
    Validate weather history data.
    
    Args:
        weather_history: DataFrame with weather history data
        
    Returns:
        Tuple: (is_valid, list of issues)
    """
    issues = []
    
    # Check if DataFrame is empty
    if weather_history.empty:
        issues.append("Weather history DataFrame is empty")
        return False, issues
    
    # Check required columns
    required_columns = ['timestamp', 'temperature']
    missing_columns = [col for col in required_columns if col not in weather_history.columns]
    if missing_columns:
        issues.append(f"Missing required columns: {missing_columns}")
    
    # Check timestamp column
    if 'timestamp' in weather_history.columns:
        # Convert to datetime if not already
        if not pd.api.types.is_datetime64_any_dtype(weather_history['timestamp']):
            try:
                pd.to_datetime(weather_history['timestamp'])
            except:
                issues.append("Timestamp column cannot be converted to datetime")
        
        # Check for duplicate timestamps
        duplicate_timestamps = weather_history['timestamp'].duplicated()
        if duplicate_timestamps.any():
            issues.append(f"Found {duplicate_timestamps.sum()} duplicate timestamps")
    
    # Check for missing values in required fields
    for col in [c for c in required_columns if c in weather_history.columns]:
        missing_values = weather_history[col].isna().sum()
        if missing_values > 0:
            issues.append(f"Column '{col}' has {missing_values} missing values")
    
    # Check for extreme temperature values
    if 'temperature' in weather_history.columns:
        if (weather_history['temperature'] < -50).any() or (weather_history['temperature'] > 60).any():
            issues.append("Temperature values outside expected range (-50 to 60 Â°C)")
    
    # Check for other weather features if available
    if 'precipitation' in weather_history.columns:
        if (weather_history['precipitation'] < 0).any():
            issues.append("Negative precipitation values found")
    
    if 'wind_speed' in weather_history.columns:
        if (weather_history['wind_speed'] < 0).any():
            issues.append("Negative wind speed values found")
    
    # Check for chronological order
    if 'timestamp' in weather_history.columns:
        timestamps = pd.to_datetime(weather_history['timestamp'])
        if not timestamps.equals(timestamps.sort_values()):
            issues.append("Weather data is not in chronological order")
    
    # Return validation result
    return len(issues) == 0, issues

def validate_outage_records(outage_records: pd.DataFrame) -> Tuple[bool, List[str]]:
    """
    Validate outage records data.
    
    Args:
        outage_records: DataFrame with outage records data
        
    Returns:
        Tuple: (is_valid, list of issues)
    """
    issues = []
    
    # Check if DataFrame is empty
    if outage_records.empty:
        issues.append("Outage records DataFrame is empty")
        return False, issues
    
    # Check required columns
    required_columns = ['component_id', 'start_time']
    missing_columns = [col for col in required_columns if col not in outage_records.columns]
    if missing_columns:
        issues.append(f"Missing required columns: {missing_columns}")
    
    # Check timestamp columns
    for col in ['start_time', 'end_time']:
        if col in outage_records.columns:
            # Convert to datetime if not already
            if not pd.api.types.is_datetime64_any_dtype(outage_records[col]):
                try:
                    pd.to_datetime(outage_records[col])
                except:
                    issues.append(f"{col} column cannot be converted to datetime")
    
    # Check for missing values in required fields
    for col in [c for c in required_columns if c in outage_records.columns]:
        missing_values = outage_records[col].isna().sum()
        if missing_values > 0:
            issues.append(f"Column '{col}' has {missing_values} missing values")
    
    # Check consistency of start/end times
    if all(col in outage_records.columns for col in ['start_time', 'end_time']):
        # Convert to datetime if needed
        start_times = pd.to_datetime(outage_records['start_time'])
        end_times = pd.to_datetime(outage_records['end_time'])
        
        # Check for end times before start times
        invalid_duration = (end_times < start_times)
        if invalid_duration.any():
            issues.append(f"Found {invalid_duration.sum()} outages with end_time before start_time")
    
    # Return validation result
    return len(issues) == 0, issues

def validate_combined_dataset(combined_dataset: pd.DataFrame) -> Tuple[bool, List[str]]:
    """
    Validate combined dataset.
    
    Args:
        combined_dataset: DataFrame with combined data
        
    Returns:
        Tuple: (is_valid, list of issues)
    """
    issues = []
    
    # Check if DataFrame is empty
    if combined_dataset.empty:
        issues.append("Combined dataset DataFrame is empty")
        return False, issues
    
    # Check required columns from different data sources
    required_columns = {
        'component': ['component_id', 'component_type'],
        'weather': ['temperature'],
        'time': ['timestamp'],
        'outage': ['outage_flag']
    }
    
    for category, cols in required_columns.items():
        missing_columns = [col for col in cols if col not in combined_dataset.columns]
        if missing_columns:
            issues.append(f"Missing {category} columns: {missing_columns}")
    
    # Check timestamp column
    if 'timestamp' in combined_dataset.columns:
        # Convert to datetime if not already
        if not pd.api.types.is_datetime64_any_dtype(combined_dataset['timestamp']):
            try:
                pd.to_datetime(combined_dataset['timestamp'])
            except:
                issues.append("Timestamp column cannot be converted to datetime")
    
    # Check consistency of outage flags
    if 'outage_flag' in combined_dataset.columns:
        if not pd.api.types.is_bool_dtype(combined_dataset['outage_flag']):
            if not set(combined_dataset['outage_flag'].unique()).issubset({0, 1, True, False}):
                issues.append("Outage flag contains values other than 0, 1, True, or False")
    
    # Return validation result
    return len(issues) == 0, issues

def check_data_completeness(
    df: pd.DataFrame,
    time_col: str = 'timestamp',
    id_col: Optional[str] = None,
    expected_frequency: str = 'D'
) -> Dict[str, Any]:
    """
    Check for completeness of time series data.
    
    Args:
        df: DataFrame with time series data
        time_col: Column with timestamps
        id_col: Column with identifiers (for panel data)
        expected_frequency: Expected data frequency
        
    Returns:
        Dict: Dictionary with completeness metrics
    """
    # Convert timestamps to datetime if needed
    if time_col in df.columns:
        timestamps = pd.to_datetime(df[time_col])
    else:
        return {
            'has_timestamp_column': False,
            'completeness_score': 0.0,
            'issues': [f"Timestamp column '{time_col}' not found"]
        }
    
    # Calculate time range
    min_time = timestamps.min()
    max_time = timestamps.max()
    time_range_days = (max_time - min_time).days
    
    # Create expected time range
    expected_range = pd.date_range(
        start=min_time,
        end=max_time,
        freq=expected_frequency
    )
    
    # If panel data (multiple IDs)
    if id_col and id_col in df.columns:
        unique_ids = df[id_col].unique()
        
        # Calculate completeness for each ID
        completeness_by_id = {}
        overall_count = 0
        expected_total = len(expected_range) * len(unique_ids)
        
        for id_val in unique_ids:
            # Get timestamps for this ID
            id_timestamps = timestamps[df[id_col] == id_val]
            
            # Calculate missing timestamps
            missing_timestamps = set(expected_range) - set(id_timestamps)
            missing_count = len(missing_timestamps)
            
            # Calculate completeness
            id_expected = len(expected_range)
            id_actual = id_expected - missing_count
            id_completeness = id_actual / id_expected if id_expected > 0 else 0.0
            
            completeness_by_id[id_val] = {
                'completeness': id_completeness,
                'missing_count': missing_count,
                'expected_count': id_expected
            }
            
            overall_count += id_actual
        
        # Calculate overall completeness
        overall_completeness = overall_count / expected_total if expected_total > 0 else 0.0
        
        return {
            'has_timestamp_column': True,
            'completeness_score': overall_completeness,
            'by_id': completeness_by_id,
            'time_range_days': time_range_days,
            'expected_frequency': expected_frequency,
            'expected_total': expected_total,
            'actual_total': overall_count
        }
    
    # If simple time series (no ID column)
    else:
        # Calculate missing timestamps
        missing_timestamps = set(expected_range) - set(timestamps)
        missing_count = len(missing_timestamps)
        
        # Calculate completeness
        expected_count = len(expected_range)
        actual_count = expected_count - missing_count
        completeness = actual_count / expected_count if expected_count > 0 else 0.0
        
        return {
            'has_timestamp_column': True,
            'completeness_score': completeness,
            'missing_count': missing_count,
            'expected_count': expected_count,
            'actual_count': actual_count,
            'time_range_days': time_range_days,
            'expected_frequency': expected_frequency
        }

def validate_input_data_compatibility(
    grid_components: pd.DataFrame,
    weather_history: pd.DataFrame,
    outage_records: pd.DataFrame
) -> Tuple[bool, List[str]]:
    """
    Validate compatibility between input datasets.
    
    Args:
        grid_components: DataFrame with grid component data
        weather_history: DataFrame with weather history data
        outage_records: DataFrame with outage records data
        
    Returns:
        Tuple: (is_compatible, list of issues)
    """
    issues = []
    
    # Check if all DataFrames are not empty
    if any(df.empty for df in [grid_components, weather_history, outage_records]):
        issues.append("One or more input DataFrames are empty")
        return False, issues
    
    # Check component IDs consistency
    if 'component_id' in grid_components.columns and 'component_id' in outage_records.columns:
        # Get component IDs from both datasets
        grid_component_ids = set(grid_components['component_id'].unique())
        outage_component_ids = set(outage_records['component_id'].unique())
        
        # Check for outages for non-existent components
        invalid_outage_ids = outage_component_ids - grid_component_ids
        if invalid_outage_ids:
            issues.append(f"Found {len(invalid_outage_ids)} outage records for components not in grid_components")
    
    # Check time range consistency
    if 'start_time' in outage_records.columns and 'timestamp' in weather_history.columns:
        # Convert to datetime if needed
        outage_start_times = pd.to_datetime(outage_records['start_time'])
        weather_timestamps = pd.to_datetime(weather_history['timestamp'])
        
        # Get time ranges
        outage_min_time = outage_start_times.min()
        outage_max_time = outage_start_times.max()
        weather_min_time = weather_timestamps.min()
        weather_max_time = weather_timestamps.max()
        
        # Check if weather data covers outage period
        if outage_min_time < weather_min_time:
            issues.append(f"Weather data starts on {weather_min_time}, but outages start on {outage_min_time}")
        
        if outage_max_time > weather_max_time:
            issues.append(f"Weather data ends on {weather_max_time}, but outages end on {outage_max_time}")
    
    # Return validation result
    return len(issues) == 0, issues

#!/usr/bin/env python3
"""
Vulnerability Analysis Module Example Script

This script demonstrates the usage of the Vulnerability Analysis Module,
showing how to load data from Module 1 and run the complete analysis pipeline.
"""

import os
import sys
import logging
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).resolve().parent.parent
sys.path.append(str(project_root))

# Import vulnerability analysis module
from gfmf.vulnerability_analysis import VulnerabilityAnalysisModule
from gfmf.data_management.data_management_module import DataManagementModule

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("logs/vulnerability_analysis_example.log", mode="w")
    ]
)

logger = logging.getLogger("vulnerability_analysis_example")


def generate_sample_data_if_needed():
    """Generate synthetic data using the Data Management Module if needed."""
    # Define paths
    processed_dir = Path("data/processed/synthetic")
    
    # Check if data already exists
    if (processed_dir / "grid_components.pkl").exists() and \
       (processed_dir / "weather_history.pkl").exists() and \
       (processed_dir / "outage_records.pkl").exists() and \
       (processed_dir / "combined_dataset.pkl").exists():
        logger.info("Synthetic data files already exist. Skipping data generation.")
        return
    
    logger.info("Generating synthetic data using the Data Management Module...")
    
    # Initialize the Data Management Module
    data_manager = DataManagementModule()
    
    # Generate synthetic data
    synthetic_data = data_manager.generate_synthetic_data(
        grid_params={
            'num_nodes': 100,
            'num_lines': 150
        },
        weather_params={
            'num_weather_stations': 10,
            'sim_start_date': "2023-01-01",
            'sim_end_date': "2023-12-31"
        },
        outage_params={
            'num_outages': 50
        }
    )
    
    # Create processed directory if it doesn't exist
    os.makedirs(processed_dir, exist_ok=True)

    # Map data types to expected file names
    file_name_map = {
        'grid': 'grid_components.pkl',
        'weather': 'weather_history.pkl',
        'outage': 'outage_records.pkl',
        'combined': 'combined_dataset.pkl'
    }
    
    # Save the synthetic data with the expected file names
    for data_type, data in synthetic_data.items():
        if data_type in file_name_map:
            file_path = os.path.join(processed_dir, file_name_map[data_type])
            with open(file_path, 'wb') as f:
                import pickle
                pickle.dump(data, f)
            logger.info(f"Saved {data_type} to {file_path}")
    
    logger.info(f"Synthetic data generated and saved to {processed_dir}")


def run_vulnerability_analysis():
    """Run the full vulnerability analysis pipeline."""
    logger.info("Starting vulnerability analysis example")
    
    # Generate sample data if needed
    generate_sample_data_if_needed()
    
    # Initialize the Vulnerability Analysis Module
    logger.info("Initializing Vulnerability Analysis Module")
    vulnerability_analyzer = VulnerabilityAnalysisModule()
    
    # Load data from Module 1
    logger.info("Loading input data from Module 1")
    input_data = vulnerability_analyzer.load_input_data(use_synthetic=True)
    
    # Log basic data statistics
    logger.info(f"Loaded {len(input_data['grid_components'])} grid components")
    logger.info(f"Loaded {len(input_data['weather_history'])} weather records")
    logger.info(f"Loaded {len(input_data['outage_records'])} outage records")
    
    # Run the vulnerability analysis pipeline
    logger.info("Running vulnerability analysis pipeline")
    analysis_results = vulnerability_analyzer.run_analysis(use_synthetic=True)
    
    return vulnerability_analyzer, analysis_results


def display_vulnerability_scores(analysis_results):
    """Display vulnerability scores for grid components."""
    vulnerability_scores = analysis_results['vulnerability_scores']
    
    print("\n===== Component Vulnerability Scores =====")
    print(f"Total components analyzed: {len(vulnerability_scores)}")
    
    # Display top 10 most vulnerable components
    top_vulnerable = vulnerability_scores.sort_values('vulnerability_score', ascending=False).head(10)
    print("\nTop 10 Most Vulnerable Components:")
    for i, (_, row) in enumerate(top_vulnerable.iterrows(), 1):
        component_type = row.get('component_type', 'Unknown')
        print(f"{i}. Component ID: {row['component_id']} ({component_type}) - "
              f"Vulnerability Score: {row['vulnerability_score']:.4f}, "
              f"Risk Category: {row.get('risk_category', 'Unknown')}")
    
    # Display vulnerability statistics by component type
    if 'component_type' in vulnerability_scores.columns:
        print("\nVulnerability Statistics by Component Type:")
        type_stats = vulnerability_scores.groupby('component_type')['vulnerability_score'].agg(
            ['count', 'mean', 'std', 'min', 'max']
        )
        print(type_stats)
    
    # Create vulnerability score distribution plot
    plt.figure(figsize=(10, 6))
    plt.hist(vulnerability_scores['vulnerability_score'], bins=20, alpha=0.7, color='skyblue')
    plt.title('Distribution of Component Vulnerability Scores')
    plt.xlabel('Vulnerability Score')
    plt.ylabel('Number of Components')
    plt.grid(alpha=0.3)
    plt.savefig('examples/results/vulnerability_distribution.png', dpi=300, bbox_inches='tight')
    print("\nVulnerability distribution plot saved to 'examples/results/vulnerability_distribution.png'")


def display_environmental_threats(analysis_results):
    """Display environmental threat analysis results."""
    threat_profiles = analysis_results['threat_profiles']
    
    print("\n===== Environmental Threat Analysis =====")
    print(f"Analyzed threat types: {list(threat_profiles.keys())}")
    
    # Display stats for each threat type
    for threat_type, profile_df in threat_profiles.items():
        # Calculate basic threat statistics
        threshold_exceedances = profile_df['exceeds_threshold'].sum()
        max_threat_level = profile_df['threat_level'].max()
        avg_threat_level = profile_df['threat_level'].mean()
        
        # Display threat information
        print(f"\n{threat_type.replace('_', ' ').title()}:")
        print(f"  Threshold: {profile_df['threshold'].iloc[0]:.2f}")
        print(f"  Threshold exceedances: {threshold_exceedances} out of {len(profile_df)} records "
              f"({100 * threshold_exceedances / len(profile_df):.1f}%)")
        print(f"  Maximum threat level: {max_threat_level:.4f}")
        print(f"  Average threat level: {avg_threat_level:.4f}")
        
        if 'cumulative_threat' in profile_df.columns:
            max_cumulative = profile_df['cumulative_threat'].max()
            print(f"  Maximum cumulative threat (7-day): {max_cumulative:.4f}")
    
    # Create temporal threat visualization
    # Focus on high temperature threat as an example
    if 'high_temperature' in threat_profiles:
        ht_profile = threat_profiles['high_temperature']
        plt.figure(figsize=(12, 6))
        
        # Plot threat level over time
        plt.plot(ht_profile['timestamp'], ht_profile['threat_level'], 
                 color='red', label='Threat Level')
        
        # Add threshold exceedance markers
        exceedance_points = ht_profile[ht_profile['exceeds_threshold'] > 0]
        plt.scatter(exceedance_points['timestamp'], exceedance_points['threat_level'], 
                    color='darkred', marker='o', s=30, label='Threshold Exceedance')
        
        plt.title('High Temperature Threat Level Over Time')
        plt.xlabel('Date')
        plt.ylabel('Threat Level')
        plt.legend()
        plt.grid(alpha=0.3)
        plt.savefig('examples/results/temperature_threat.png', dpi=300, bbox_inches='tight')
        print("\nHigh temperature threat plot saved to 'examples/results/temperature_threat.png'")


def display_correlation_analysis(analysis_results):
    """Display correlation analysis results."""
    correlation_matrices = analysis_results['correlation_matrices']
    
    print("\n===== Correlation Analysis Results =====")
    
    # Display top correlations with outage events
    if 'spearman' in correlation_matrices:
        corr_matrix = correlation_matrices['spearman']
        
        # Find outage-related columns
        outage_cols = [col for col in corr_matrix.columns if 'outage' in col]
        
        # Find environmental columns
        env_cols = [col for col in corr_matrix.columns if any(
            term in col for term in ['temperature', 'precipitation', 'wind', 'threat']
        )]
        
        if outage_cols and env_cols:
            print("\nTop Environmental Factors Correlated with Outages (Spearman):")
            
            for outage_col in outage_cols:
                # Get correlations with this outage variable
                correlations = corr_matrix.loc[env_cols, outage_col].sort_values(ascending=False)
                
                print(f"\nCorrelations with {outage_col}:")
                for var, corr in correlations.head(5).items():
                    print(f"  {var}: {corr:.4f}")
    
    # Create correlation heatmap visualization
    if 'pearson' in correlation_matrices:
        plt.figure(figsize=(12, 10))
        
        # Select a subset of variables for better visualization
        key_vars = []
        
        # Add outage variables
        key_vars.extend([col for col in correlation_matrices['pearson'].columns 
                        if 'outage' in col][:3])
        
        # Add top correlated environmental variables
        env_vars = [col for col in correlation_matrices['pearson'].columns if any(
            term in col for term in ['temperature', 'precipitation', 'wind', 'threat']
        )]
        
        # Add some key environmental variables
        for term in ['temperature_max', 'precipitation_max', 'wind_speed_max', 
                     'high_temperature_max', 'high_wind_max']:
            matching = [col for col in env_vars if term in col]
            if matching:
                key_vars.append(matching[0])
        
        # Ensure we don't have too many variables for the heatmap
        key_vars = key_vars[:15]
        
        # Create heatmap
        if key_vars:
            selected_corr = correlation_matrices['pearson'].loc[key_vars, key_vars]
            
            # Generate heatmap
            import seaborn as sns
            sns.heatmap(
                selected_corr, 
                annot=True, 
                cmap='coolwarm', 
                vmin=-1, 
                vmax=1, 
                linewidths=0.5,
                fmt='.2f'
            )
            plt.title('Pearson Correlation Heatmap (Key Variables)')
            plt.tight_layout()
            plt.savefig('examples/results/correlation_heatmap.png', dpi=300, bbox_inches='tight')
            print("\nCorrelation heatmap saved to 'examples/results/correlation_heatmap.png'")


def main():
    """Main function to run the example."""
    # Create directories for results if they don't exist
    os.makedirs("examples/results", exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    
    # Run vulnerability analysis
    vulnerability_analyzer, analysis_results = run_vulnerability_analysis()
    
    # Display results
    display_vulnerability_scores(analysis_results)
    display_environmental_threats(analysis_results)
    display_correlation_analysis(analysis_results)
    
    print("\n===== Vulnerability Analysis Complete =====")
    print(f"Full results saved to {vulnerability_analyzer.output_data_path}")
    print("Example visualizations saved to 'examples/results/'")


if __name__ == "__main__":
    main()
